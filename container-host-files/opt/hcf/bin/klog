#!/bin/bash

set -e

KLOG=${HOME}/klog

if [ "$1" == "-h" ]; then
    cat <<EOF
usage: $0 [-f] [-v] [INSTANCe_ID]

  -f  forces fetching of all logs even if a cache already exists
  -v  prints verbose output

  INSTANCE_ID defaults to "hcf"
EOF
    exit
fi

FORCE=0
if [ "$1" == "-f" ]; then
    shift
    FORCE=1
fi

VErBOSE=0
if [ "$1" == "-v" ]; then
    shift
    VERBOSE=1
fi

NS=${1-hcf}
DONE=${KLOG}/${NS}/done

[ "${FORCE}" == "1" ] && rm ${DONE} 2> /dev/null || true

if [ ! -f ${DONE} ]; then
    mkdir -p ${KLOG}/${NS}
    rm -rf ${KLOG}/${NS}/*

    PODS=$(kubectl get pods --namespace ${NS} -o name | sed 's/pod\///')

    for POD in ${PODS}; do
        if $(kubectl exec ${POD} --namespace ${NS} -- bash -c "[ -d /var/vcap/sys/log ]"); then
            DIR=${KLOG}/${NS}/${POD%-*-*}
            mkdir -p ${DIR}
            cd ${DIR}
            echo Fetching logs for ${POD}
            kubectl exec ${POD} --namespace ${NS} -- bash -c "cd /var/vcap/sys/log && tar cf - *" | tar xf -
        fi
    done
    gunzip -r ${KLOG}
    touch ${DONE}
fi

NEWLINE=0
function lookfor {
    read PATTERN

    cd ${KLOG}
    if grep -c -r -F "${PATTERN}" > .grep; then
        [ "${NEWLINE}" == "1" ] && echo
        NEWLINE=1

        echo ">>> ${PATTERN}"
        if [ "${VERBOSE}" == "1" ]; then
            echo
            grep -v :0$ .grep
        fi

        while read INFO; do
            echo ${INFO}
        done
    fi
}

lookfor <<EOF
[ERR] agent: failed to sync remote state: No known Consul servers

This happened on a consul agent that came up before the consul server,
then abandoned the server as non-functional and then had no friends to
play with.
EOF


lookfor <<EOF
[ERROR] WSREP: SST failed: 1 (Operation not permitted)

SST has happened twice and should only ever happen once.
EOF


lookfor <<EOF
[Warning] InnoDB: Cannot open table mysql/gtid_slave_pos from the internal data dictionary of InnoDB

SST has happened twice and should only ever happen once
EOF


lookfor <<EOF
[Warning] WSREP: no nodes coming from prim view, prim not possible

Nodes all shut down improperly, MySQL must be manually rebootstrapped.
EOF


lookfor <<EOF
[Warning] WSREP: Failed to prepare for incremental state transfer

IST addresses are misconfigured, likely a <= 4.0.1 cluster, manual
patch must be applied and MySQL must be recovered manually.

Not sure exactly when/why this occurs. Perhaps when the primary node
has gone away (as was the case here). The only restitution seems to be
to full recover the mysql node (and ensure the IST patch is done):

monit stop all # Check for lingering processes and kill them
rm -rf /var/vcap/store/mysql/
/var/vcap/jobs/mysql/bin/pre-start
monit start all
EOF


# etcd pod /var/vcap/sys/log/etcd/etcd_ctl.err.log
lookfor <<EOF
Error:  cannot sync with the cluster using endpoints https://etcd-

These point to an etcd that came up in a weird way and thinks there
are other nodes when there are none.

Mark: restarting the pod didn't help. I had to go into the pod, run
monit stop all, then delete /var/vcap/store/etcd (and then restarting
the pod) to get it to work again. This was a single node (non-HA)
cluster; no idea how one would recover in an HA scenario. Also, that
killed all the data.
EOF


# etcd pod /var/vcap/sys/log/etcd/etcd.stderr.log
# duplicate of previous error?
lookfor <<EOF
etcdserver: publish error: etcdserver: request timed out

These point to an etcd that came up in a weird way and thinks there
are other nodes when there are none.

Mark: restarting the pod didn't help. I had to go into the pod, run
monit stop all, then delete /var/vcap/store/etcd (and then restarting
the pod) to get it to work again. This was a single node (non-HA)
cluster; no idea how one would recover in an HA scenario. Also, that
killed all the data.
EOF


# mysql pods /var/vcap/sys/log/mysql/mysql.err.log
lookfor <<EOF
SST disabled due to danger of data loss. Verify data and bootstrap the cluster

IST is probably disabled so it's falling back to this, this happens on
4.0.1 clusters, the fix is complicated as it involves manually
patching
EOF


# consul_agent.stdout.log in the consul roles
lookfor <<EOF
Only one node should be in bootstrap mode, not adding Raft peer.

This occurs when two consul nodes race to bootstrap. Usually deleting
the consul pods (so HCP makes new ones) fixes this.
EOF


# Stager log inside the cc-bridge:
lookfor <<EOF
"CellCommunicationError","message":"unable to communicate to compatible cells"

This occurs whet the diego cells don't reconnect properly to the bbs
(diego-database). A restart of the diego-cell pods should fix it (and
you should make sure you only have one instance).
EOF
